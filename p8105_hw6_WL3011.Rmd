---
title: "p8105_hw6_WL3011"
author: "Weiqi Liang"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, message = FALSE, results='hide'}
library(tidyverse)
library(dplyr)
library(knitr)
set.seed(1)
library(ggplot2)
library(broom)
library(purrr)
library(modelr)
```

## Problem 1

```{r, message = FALSE, results='hide'}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

Dataset **weather_df** has `r nrow(weather_df)` rows and `r ncol(weather_df)` columns, showing the 2017 Central Park weather data. Each row represents a single day's weather data.

First generate 5000 bootstrap samples to estimate the two quantities:

${\hat r^2}$ (R-squared)

$\log ({\hat \beta _0} \cdot {\hat \beta _1})$ (log-transformed product of coefficients)

```{r}
# Bootstrap 
bootstrap_results = weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    # R^2
    r_squared = map_dbl(models, \(model) broom::glance(model) |> pull(r.squared)),
    # log(Beta0 * Beta1)
    log_beta_product = map_dbl(models, \(model) {
      coefs <- broom::tidy(model) |> pull(estimate)
      log(coefs[1] * coefs[2])
    })
  ) |> 
  select(-strap, -models)
```

Create plots of the bootstrap distributions for ${\hat r^2}$ and $\log ({\hat \beta _0} \cdot {\hat \beta _1})$.

```{r}
ci_r_squared = quantile(bootstrap_results |> pull(r_squared), probs = c(0.025, 0.975))
ci_log_beta_product = quantile(bootstrap_results |> pull(log_beta_product), probs = c(0.025, 0.975))

# R^2 
ggplot(bootstrap_results, aes(x = r_squared)) +
  geom_density(fill = "pink", alpha = 0.2) +
  geom_vline(xintercept = ci_r_squared,
             color = "red", linetype = "dashed") +
  labs(title = "Bootstrap Density of R^2",
       x = "R^2", y = "Density") +
  theme_minimal()
```

Most of the ${\hat r^2}$ values are concentrated in the region close to the median value 0.91, and the overall distribution is close to symmetry.

```{r}
# log(Beta0 * Beta1) 
ggplot(bootstrap_results, aes(x = log_beta_product)) +
  geom_density(fill = "skyblue", alpha = 0.2) +
  geom_vline(xintercept = ci_log_beta_product,
             color = "red", linetype = "dashed") +
  labs(title = "Bootstrap Density of log(Beta0 * Beta1) ",
       x = "log(Beta0 * Beta1) ", y = "Density") +
  theme_minimal()
```

The $\log ({\hat \beta _0} \cdot {\hat \beta _1})$ are mainly concentrated in the range of pairs from 1.95 to 2.10. The symmetry and compact shape of the distribution indicate that the measurement is relatively stable across multiple samples.

Using the bootstrap results, compute the 2.5% and 97.5% quantiles for both quantities.

```{r}
kable(ci_r_squared, 
      col.names = c("quantiles", "value"),
      digits = 4)
kable(ci_log_beta_product, 
      col.names = c("quantiles", "value"),
      digits = 4)
```

The 95% CI for ${\hat r^2}$ is (`r sprintf("%.4f", ci_r_squared[1])`, `r sprintf("%.4f", ci_r_squared[2])`).

The 95% CI for $\log ({\hat \beta _0} \cdot {\hat \beta _1})$ is (`r sprintf("%.4f", ci_log_beta_product[1])`, `r sprintf("%.4f", ci_log_beta_product[2])`).

## Problem 2

First, perform data cleansing as required.

```{r, message = FALSE, warning=FALSE}
homicide_data = read_csv("./homicide-data.csv", 
           na = c("NA", ".", "")) |>
           janitor::clean_names() |>
  mutate(city_state = paste(city, state, sep = ", "),
         victim_age = as.numeric(victim_age),
         resolved = ifelse(disposition == "Closed by arrest", 1, 0)) |>
  filter(
    !(city_state %in% c(
      "Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"
      )),
    victim_race %in% c("White", "Black"),
    !is.na(victim_age)) 
```

Then create logistic regression model for Baltimore, MD.

```{r}
baltimore_data = homicide_data |>
  filter(city_state == "Baltimore, MD")

baltimore_model = glm(
  resolved ~ victim_age + victim_sex + victim_race,
  data = baltimore_data,
  family = binomial
)

# baltimore OR
baltimore_or = broom::tidy(baltimore_model) |>
  mutate(
    odds_ratio = exp(estimate),
    lower_ci = exp(estimate - 1.96 * std.error),
    upper_ci = exp(estimate + 1.96 * std.error)
  ) |>
  filter(term == "victim_sexMale") |>
select(term, odds_ratio, lower_ci, upper_ci) 

kable(baltimore_or, digits = 4)
```

The Odds Ratio(OR) is 0.4255, meaning that male victims are significantly less likely to have their cases resolved than female victims. At a 95% confidence level, the actual OR value may fall between 0.3246 and 0.5579. CI does not contain 1, so this result can be considered statistically significant.

```{r}
# all cities OR
city_or = homicide_data |>
  group_by(city_state) |>
  nest() |>
  mutate(
    models = map(data, \(df) glm(resolved ~ victim_age + victim_sex + victim_race, family = binomial, data = df)),
    or_results = map(models, \(model) broom::tidy(model) |>
                       mutate(
                         odds_ratio = exp(estimate),
                         lower_ci = exp(estimate - 1.96 * std.error),
                         upper_ci = exp(estimate + 1.96 * std.error)
                       ))
  ) |>
  unnest(or_results) |>
  filter(term == "victim_sexMale") |>
  arrange(desc(odds_ratio)) |>
  select(city_state, odds_ratio, lower_ci, upper_ci) 

kable(city_or, digits = 4)
```

```{r, message = FALSE, warning=FALSE,  fig.width = 9, fig.height = 8, fig.cap="Figure 3. Adjusted Odds Ratios by City"}
ggplot(city_or, aes(x = reorder(city_state, odds_ratio), y = odds_ratio)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +
  coord_flip() +
  labs(
    x = "City",
    y = "Odds Ratio (Male vs Female)"
  ) +
  theme_minimal()
```

The vast majority of cities have an OR of less than 1, indicating that male victims have a lower probability of case resolution than female victims. However, cities such as San Francisco, CA, and Oakland, CA have an OR close to 1 and a confidence interval that crosses 1. This means that there is no significant difference in the likelihood of cases being resolved between men and women in these cities. A few cities (such as Albuquerque, NM, and Stockton, CA) have an OR greater than 1, indicating a higher probability of case resolution for men than for women. 

In addition, some cities (such as Albuquerque, NM, and San Bernardino, CA) have wide confidence intervals, indicating that these cities may have insufficient data and less accurate estimates.

## Problem 3

```{r, message = FALSE, warning=FALSE}
birthweight_data = read_csv("./birthweight.csv", 
           na = c("NA", ".", "")) |>
           janitor::clean_names() |>
  mutate(
    babysex = factor(case_when(
      babysex == 1 ~ "male",
      babysex == 2 ~ "female")),
    frace = factor(case_when(
      frace == 1 ~ "White",
      frace == 2 ~ "Black",
      frace == 3 ~ "Asian",
      frace == 4 ~ "Puerto Rican",
      frace == 8 ~ "Other",
      frace == 9 ~ "Unknown")),
    mrace = factor(case_when(
      mrace == 1 ~ "White",
      mrace == 2 ~ "Black",
      mrace == 3 ~ "Asian",
      mrace == 4 ~ "Puerto Rican",
      mrace == 8 ~ "Other")),
    malform = factor(case_when(
      malform == 0 ~ "absent",
      malform == 1 ~ "present"))) |>
  drop_na()
```

Stepwise regression is performed, and the optimal variable set is selected automatically through AIC.

```{r, message = FALSE, warning=FALSE}
# Stepwise regression
library(MASS)

full_model = lm(bwt ~ ., data = birthweight_data)
null_model = lm(bwt ~ 1, data = birthweight_data)
selected_model = stepAIC(
  object = null_model,         
  scope = list(lower = null_model, upper = full_model),  
  direction = "both",         
  trace = FALSE               
)

detach("package:MASS", unload = TRUE)

summary(selected_model)
```

```{r, message = FALSE, warning=FALSE}
hypothesis_model = lm(bwt ~ bhead + blength + mrace + delwt + gaweeks + 
    smoken + ppbmi + babysex + parity + wtgain + fincome, data = birthweight_data)

# Residuals vs. Fitted Values Plot
birthweight_data = birthweight_data |>
  add_predictions(hypothesis_model) |>
  add_residuals(hypothesis_model)

ggplot(birthweight_data, aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5, color = "orange") +
  geom_smooth(method = "loess", color = "skyblue", se = FALSE) +
  labs(title = "Residuals vs Fitted Values",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()
```

```{r}
model1 = lm(bwt ~ blength + gaweeks, data = birthweight_data)
model2 = lm(bwt ~ bhead + blength + babysex + 
              bhead * blength + blength * babysex + bhead * babysex +
              bhead * blength * babysex, data = birthweight_data)

# Evaluate models with cross-validation
cv_results = crossv_mc(birthweight_data, n = 100) |>
  mutate(
    hypothesis_model = map(train, ~hypothesis_model),
    model1 = map(train, ~model1),
    model2 = map(train, ~model2),
    rmse_hypothesis = map2_dbl(hypothesis_model, test, \(Model, df) rmse(model = Model, data = df)),
    rmse_model1 = map2_dbl(model1, test, \(Model, df) rmse(model = Model, data = df)),
    rmse_model2 = map2_dbl(model2, test, \(Model, df) rmse(model = Model, data = df)),
  )

# Summarize average RMSE for each model
cv_summary = cv_results |>
  summarize(
    hypothesis_rmse = mean(rmse_hypothesis),
    model1_rmse = mean(rmse_model1),
    model2_rmse = mean(rmse_model2)
  )

kable(cv_summary, digits = 4)
```

The **Hypothesis Model** had the lowest RMSE, indicating that it captured more key variables and information, and had the best predictive performance for birth weight. Although Model 2 improves performance by including interaction terms, it is still slightly inferior to Hypothesis Model due to increased complexity. Model 1 is the simplest model, but it misses many important predictors and has a large prediction error.



